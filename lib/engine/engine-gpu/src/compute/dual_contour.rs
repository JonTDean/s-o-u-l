//! Dual-Contouring GPU pass (guaranteed fallback path).
//!
//! This module owns **all** CPU-side resources (buffers, bind-groups,
//! pipelines) required by the fallback Dual-Contouring path that runs on
//! _every_ GPU.  Because these resources live in the **render-world** we
//! can safely access Bevy’s `RenderDevice`, which becomes available after
//! the `WgpuRenderPlugin` has built the underlying `wgpu::Device`.
//!
//! ## ⚠️  Initialisation Order
//! Earlier revisions used an `impl FromWorld` for `MeshletBuffers` and
//! initialised the resource via `init_resource::<MeshletBuffers>()` inside
//! our plug-in’s `build()` method.  That **panicked** on some drivers
//! because the `RenderDevice` was not yet present when `FromWorld` ran.
//!
//! The new design removes the `FromWorld` impl entirely.  Instead, the
//! compute plug-in inserts the resource from an explicit **startup system**
//! (`init_meshlet_buffers`) that Bevy schedules _after_ the render
//! back-end is fully initialised.  This guarantees that `RenderDevice` is
//! available, eliminates the panic, and keeps the code single-thread safe
//! (the system runs single-threaded at startup).
//!
//! ## Thread-Safety & Multi-Threading
//! * `MeshletBuffers` is `Send + Sync` (through `Buffer` handles), so it
//!   can be accessed from any render graph node in parallel.
//! * The compute and graphics passes write and read the buffers on the GPU
//!   only – no host locks required.
//! * CPU writes happen through `RenderQueue::write_buffer`, which queues
//!   the transfer on the command stream and is therefore thread-safe.
//!
//! © 2025 Obaven Inc. — Apache-2.0 OR MIT

use bevy::prelude::*;
use bevy::render::{
    render_asset::RenderAssets,
    render_graph::{Node, NodeRunError, RenderGraphContext},
    renderer::{RenderContext, RenderDevice, RenderQueue},
    render_resource::*,
    texture::GpuImage,
};
use bytemuck::{Pod, Zeroable};

use crate::{
    graph::ExtractedGpuSlices,
    pipelines::GpuPipelineCache,
    plugin::GlobalStateTextures,
};

/* ──────────────────────────────────────────────────────────────────── */
/* Vertex format                                                       */
/* ──────────────────────────────────────────────────────────────────── */
#[repr(C)]
#[derive(Clone, Copy, Pod, Zeroable, Debug)]
pub struct Vertex {
    /// World-space position of the generated triangle vertex.
    pub pos: Vec3,
    /// World-space interpolated normal (len ≈ 1).  Unused in the fallback
    /// path but kept for parity with the mesh-shader fast-path.
    pub nrm: Vec3,
    /// Future material id (atlas index).
    pub mat: u32,
}

/* ──────────────────────────────────────────────────────────────────── */
/*  Constants                                                           */
/* ──────────────────────────────────────────────────────────────────── */
/// Work-group size for the compute kernel.
pub const WG_SIZE: (u32, u32, u32) = (8, 8, 8);
/// Maximum number of quads generated by one voxel (upper bound).
pub const MAX_QUADS: u32 = 5;
/// Stride of a single [`Vertex`] in bytes (std430).
pub const BYTES_PER_VERT: u64 = std::mem::size_of::<Vertex>() as u64;
/// Conservative upper bound for voxels processed per frame (1024²).
pub const MAX_VOXELS: u32 = 1024 * 1024;

/* ──────────────────────────────────────────────────────────────────── */
/*  Per-frame GPU scratch buffers                                       */
/* ──────────────────────────────────────────────────────────────────── */
/// Three GPU buffers allocated once per session:
/// * **vertices**  – append-only storage buffer written by the compute pass.
/// * **counter**   – atomic counter (u32) tracking the vertex count.
/// * **indirect**  – `DrawIndirect` struct consumed by the graphics pass.
#[derive(Resource)]
pub struct MeshletBuffers {
    pub vertices: Buffer,
    pub counter: Buffer,
    pub indirect: Buffer,
    /// Total capacity of `vertices` in **bytes** (helps with sanity checks).
    pub capacity: u64,
}

impl MeshletBuffers {
    /// Allocate all GPU resources for **at most** `voxel_cap` voxels.
    pub fn new(device: &RenderDevice, voxel_cap: u32) -> Self {
        let max_vertices = voxel_cap as u64 * MAX_QUADS as u64 * 6;
        let size_bytes = max_vertices * BYTES_PER_VERT;

        let vertices = device.create_buffer(&BufferDescriptor {
            label: Some("dc.vertices"),
            size: size_bytes,
            usage: BufferUsages::STORAGE | BufferUsages::VERTEX | BufferUsages::COPY_SRC,
            mapped_at_creation: false,
        });
        let counter = device.create_buffer(&BufferDescriptor {
            label: Some("dc.counter"),
            size: 4,
            usage: BufferUsages::STORAGE | BufferUsages::COPY_DST | BufferUsages::COPY_SRC,
            mapped_at_creation: false,
        });
        let indirect = device.create_buffer(&BufferDescriptor {
            label: Some("dc.indirect"),
            size: std::mem::size_of::<DrawIndirect>() as u64,
            usage: BufferUsages::INDIRECT | BufferUsages::COPY_DST | BufferUsages::COPY_SRC,
            mapped_at_creation: false,
        });

        Self { vertices, counter, indirect, capacity: size_bytes }
    }
}

/* ──────────────────────────────────────────────────────────────────── */
/*  Indirect draw arguments struct                                      */
/* ──────────────────────────────────────────────────────────────────── */
#[repr(C)]
#[derive(Clone, Copy)]
struct DrawIndirect {
    vertex_count: u32,
    instance_count: u32,
    first_vertex: u32,
    first_instance: u32,
}

/* ──────────────────────────────────────────────────────────────────── */
/* Render-graph node                                                   */
/* ──────────────────────────────────────────────────────────────────── */
#[derive(Debug)]
pub struct DualContourNode;

impl Node for DualContourNode {
    fn run(
        &self,
        _graph: &mut RenderGraphContext,
        ctx: &mut RenderContext,
        world: &World,
    ) -> Result<(), NodeRunError> {
        /* 0 ░ Early-out until all required resources are available */
        let (Some(device), Some(queue)) = (
            world.get_resource::<RenderDevice>(),
            world.get_resource::<RenderQueue>(),
        ) else {
            return Ok(());
        };

        let slices = world.resource::<ExtractedGpuSlices>();
        if slices.0.is_empty() {
            return Ok(());
        }
        let images = world.resource::<RenderAssets<GpuImage>>();
        let tex = world.resource::<GlobalStateTextures>();
        let cache = world.resource::<GpuPipelineCache>();
        let pipelines = world.resource::<PipelineCache>();
        let mesh = world.resource::<MeshletBuffers>();

        /* 1 ░ Compute pipeline */
        let Some(&pid) = cache.map.get("dual_contour") else { return Ok(()); };
        let Some(pipe) = pipelines.get_compute_pipeline(pid) else { return Ok(()); };

        /* 2 ░ Bind-group */
        let layout = BindGroupLayout::from(pipe.get_bind_group_layout(0));
        let entries = [
            BindGroupEntry {
                binding: 0,
                resource: BindingResource::TextureView(&images.get(&tex.ping).unwrap().texture_view),
            },
            BindGroupEntry {
                binding: 1,
                resource: BindingResource::TextureView(&images.get(&tex.pong).unwrap().texture_view),
            },
            BindGroupEntry { binding: 2, resource: mesh.vertices.as_entire_binding() },
            BindGroupEntry { binding: 3, resource: mesh.counter.as_entire_binding() },
        ];
        let bind = device.create_bind_group(Some("dc.bind0"), &layout, &entries);

        /* 3 ░ Reset atomics + indirect */
        queue.write_buffer(&mesh.counter, 0, bytemuck::bytes_of(&0u32));
        queue.write_buffer(&mesh.indirect, 4, bytemuck::bytes_of(&1u32)); // instance_count
        queue.write_buffer(&mesh.indirect, 8, bytemuck::bytes_of(&0u32));
        queue.write_buffer(&mesh.indirect, 12, bytemuck::bytes_of(&0u32));

        /* 4 ░ Work-group count derives from the **largest** slice */
        let w = slices.0.iter().map(|s| s.0.size.x).max().unwrap();
        let h = slices.0.iter().map(|s| s.0.size.y).max().unwrap();
        let gx = (w + WG_SIZE.0 - 1) / WG_SIZE.0;
        let gy = (h + WG_SIZE.1 - 1) / WG_SIZE.1;

        /* 5 ░ Dispatch */
        {
            let mut pass = ctx.command_encoder().begin_compute_pass(&ComputePassDescriptor {
                label: Some("dual_contour.compute"),
                timestamp_writes: None,
            });
            pass.set_pipeline(pipe);
            pass.set_bind_group(0, &bind, &[]);
            pass.dispatch_workgroups(gx, gy, 1);
        }

        /* 6 ░ Copy vertex-count → indirect */
        ctx.command_encoder()
            .copy_buffer_to_buffer(&mesh.counter, 0, &mesh.indirect, 0, 4);

        Ok(())
    }
}